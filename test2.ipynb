{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the data\n",
    "dataset_path = os.path.join(data_folder,'training_set.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data_handling as handling\n",
    "\n",
    "squad_dataset = handling.RawSquadDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset.raw_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(embedding_dimension=50, unk_token=\"[UNK]\", pad_token=\"[PAD]\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    try:\n",
    "        embedding_model : KeyedVectors = gloader.load(model)\n",
    "\n",
    "        # Build the unknown vector as the mean of all vectors\n",
    "        # (if the mean is already present, use a random vector)\n",
    "        assert unk_token not in embedding_model, f\"{unk_token} key already present\"\n",
    "        unk = np.mean(embedding_model.vectors, axis=0)\n",
    "        if unk in embedding_model.vectors:\n",
    "            mins = np.min(embedding_model.vectors, axis=0)\n",
    "            maxs = np.max(embedding_model.vectors, axis=0)\n",
    "            unk = (maxs - mins) * np.random.rand(embedding_dimension) + mins\n",
    "        assert unk not in embedding_model.vectors, f\"{unk_token} value already present\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Build the pad vector as a zero vector\n",
    "        assert pad_token not in embedding_model, f\"{pad_token} key already present\"\n",
    "        pad = np.zeros((embedding_model.vectors.shape[1],))\n",
    "        assert pad not in embedding_model.vectors, f\"{pad_token} value already present\"\n",
    "\n",
    "        print(unk.shape)\n",
    "        print(pad.shape)\n",
    "        embedding_model.add_vectors([unk_token,pad_token], [unk,pad])\n",
    "        \n",
    "        # embedding_model.add_vectors(pad_token, pad)\n",
    "\n",
    "        # Extract a mapping from keys to indexes\n",
    "        vocab = dict(\n",
    "            zip(embedding_model.index_to_key , range(len(embedding_model.index_to_key)))\n",
    "        )\n",
    "\n",
    "        return embedding_model, vocab\n",
    "    except Exception as e:\n",
    "        print(\"Invalid embedding model name.\")\n",
    "        raise e\n",
    "\n",
    "model, vocab = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.key_to_index == vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_index('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in vocab.items():\n",
    "    if v == 400001:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Lowercase, Sequence, Strip, StripAccents\n",
    "from tokenizers.pre_tokenizers import Punctuation\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "hf_dataset = Dataset.from_pandas(squad_dataset.raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(vocab,unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = Sequence([StripAccents(), Lowercase(), Strip()])\n",
    "tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "# tokenizer.enable_padding(\n",
    "#         direction=\"right\", pad_id=vocab['[PAD]'], pad_type_id=1, pad_token='[PAD]'\n",
    "#     )\n",
    "\n",
    "def convert_to_features(batch):\n",
    "    #print(example_batch['context'][0])\n",
    "    encodings = tokenizer.encode_batch(batch['context'])\n",
    "    # target_encodings = tokenizer.encode(example_batch['target_text'], pad_to_max_length=True, max_length=16)\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': [e.ids for e in encodings], \n",
    "        'attention_mask': [e.attention_mask for e in encodings],\n",
    "        'offset':[e.offsets for e in encodings]\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "hf_dataset2 = hf_dataset.map(convert_to_features,batched=True)\n",
    "\n",
    "\n",
    "\n",
    "# hf_dataset.set_transform(pad_batch,output_all_columns=False)\n",
    "# 'label_token' : [list(e.char_to_token(e.label_char[0],3),e.char_to_token(e.label_char[0],3)) for e in encodings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_dataset2[4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    tokenizer.enable_padding(\n",
    "        direction=\"right\", pad_id=vocab['[PAD]'], pad_type_id=0, pad_token='[PAD]'\n",
    "    )\n",
    "    padded_encodings = tokenizer.encode_batch(batch['context'])\n",
    "\n",
    "    #print(padded_encodings[0])\n",
    "\n",
    "    encodings = {\n",
    "        'padded_ids': [e.ids for e in padded_encodings], \n",
    "        'attention_mask':[e.attention_mask for e in padded_encodings],\n",
    "        'context': batch['context']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "hf_dataset2.set_transform(pad_batch,output_all_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_dataset2[4:6])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "len(hf_dataset[6]['padded_ids']) - len(hf_dataset[0]['padded_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf_dataset[56459]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.id_to_token(1962))\n",
    "print(tokenizer.token_to_id('[PAD]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset[0,1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('squad': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
