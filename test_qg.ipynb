{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import src.globals as g\n",
    "import src.utils as utils\n",
    "\n",
    "import src.data_handler as handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(g.DATA_FOLDER,'training_set.json')\n",
    "squad_dataset = handling.RawSquadDataset(dataset_path)\n",
    "\n",
    "df = squad_dataset.train_df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = utils.load_qa_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  Tokenizer, Encoding\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD, BertNormalizer\n",
    "from tokenizers.normalizers import Sequence as NormSequence\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=g.UNK_TOKEN))\n",
    "tokenizer.normalizer = BertNormalizer(handle_chinese_chars=False) #NormSequence([NFD(), StripAccents(), Lowercase(), Strip()])    \n",
    "tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    pair=\"[SOS] $A [EOS] [SOS]:1 $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOS]\", 2),\n",
    "        (\"[EOS]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[g.PAD_TOKEN,g.UNK_TOKEN,g.SOS_TOKEN,g.EOS_TOKEN],vocab_size=40000)   #min_frequency\n",
    "\n",
    "#l = df.context.to_list() + df.answer.to_list()\n",
    "l = df.question.to_list()\n",
    "tokenizer.train_from_iterator(l,trainer=trainer) \n",
    "tokenizer.enable_padding(direction=\"right\", pad_id=tokenizer.token_to_id(g.PAD_TOKEN), pad_type_id=1, pad_token=g.PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens([g.PAD_TOKEN,g.UNK_TOKEN]) #,g.SOS_TOKEN,g.EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = df.context.to_list() + df.answer.to_list() \n",
    "s = set()\n",
    "for e in l :\n",
    "    # if 'intellectu' in e:\n",
    "    #     print(e)\n",
    "    s.update(e.split())\n",
    "\n",
    "len(s)\n",
    "        \n",
    "\n",
    "#tokenizer.encode('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?').tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for e in tokenizer.get_vocab().keys() :\n",
    "    if e not in vocab:\n",
    "        print(e)\n",
    "        n+=1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()[\"tδ\"]\n",
    "\n",
    "df[df['context'].str.contains('tδ')]\n",
    "\n",
    "for e in l :\n",
    "    if 'tδ' in e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)\n",
    "tokenizer.token_to_id('plda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = df.loc[39660,'context']\n",
    "s2 = df.loc[39601,'context']\n",
    "s1\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings: list[Encoding] = tokenizer.encode_batch([s1,s2])\n",
    "\n",
    "print([e.ids for e in encodings])\n",
    "print([e.tokens for e in encodings])\n",
    "print([e.special_tokens_mask for e in encodings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.get_vocab()['hokkien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('squad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
