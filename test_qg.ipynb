{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import src.globals as g\n",
    "import src.utils as utils\n",
    "\n",
    "import src.data_handler as handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(g.DATA_FOLDER,'training_set.json')\n",
    "squad_dataset = handling.RawSquadDataset(dataset_path)\n",
    "\n",
    "df = squad_dataset.train_df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = utils.get_Glove_model_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  Tokenizer, Encoding\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD, BertNormalizer\n",
    "from tokenizers.normalizers import Sequence as NormSequence\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=g.UNK_TOKEN))\n",
    "tokenizer.normalizer = BertNormalizer(handle_chinese_chars=False) #NormSequence([NFD(), StripAccents(), Lowercase(), Strip()])    \n",
    "tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    pair=\"[SOS] $A [EOS] [SOS]:1 $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOS]\", 2),\n",
    "        (\"[EOS]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[g.PAD_TOKEN,g.UNK_TOKEN,g.SOS_TOKEN,g.EOS_TOKEN],vocab_size=65000)   #min_frequency\n",
    "\n",
    "l = df.context.to_list() + df.answer.to_list()\n",
    "#l = df.question.to_list()\n",
    "tokenizer.train_from_iterator(l,trainer=trainer) \n",
    "tokenizer.enable_padding(direction=\"right\", pad_id=tokenizer.token_to_id(g.PAD_TOKEN), pad_type_id=1, pad_token=g.PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()\n",
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens([g.PAD_TOKEN,g.UNK_TOKEN]) #,g.SOS_TOKEN,g.EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = df.context.to_list() + df.answer.to_list() \n",
    "s = set()\n",
    "for e in l :\n",
    "    # if 'intellectu' in e:\n",
    "    #     print(e)\n",
    "    s.update(e.split())\n",
    "\n",
    "len(s)\n",
    "        \n",
    "\n",
    "#tokenizer.encode('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?').tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for e in tokenizer.get_vocab().keys() :\n",
    "    if e not in vocab:\n",
    "        # print(e)\n",
    "        n+=1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()[\"tδ\"]\n",
    "\n",
    "df[df['context'].str.contains('tδ')]\n",
    "\n",
    "for e in l :\n",
    "    if 'tδ' in e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)\n",
    "tokenizer.token_to_id('plda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['question_id']=='5726d73d708984140094d310']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = df.loc[49591]\n",
    "r2 = df.loc[49593]\n",
    "s1 = r1['context']\n",
    "s2 = r2['context']\n",
    "s1\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [r1['label_char'][0],r2['label_char'][0]]\n",
    "ends = [r1['label_char'][1],r2['label_char'][1]]\n",
    "\n",
    "starts\n",
    "ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1['answer']\n",
    "r2['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings: list[Encoding] = tokenizer.encode_batch([s1,s2])\n",
    "\n",
    "print([e.ids for e in encodings])\n",
    "print([e.attention_mask for e in encodings])\n",
    "# print([e.offsets for e in encodings])\n",
    "print([e.char_to_token(starts[i]) for i,e in enumerate(encodings)])\n",
    "print([e.char_to_token(ends[i]-1) for i,e in enumerate(encodings)])\n",
    "print([e.type_ids for e in encodings])\n",
    "print([e.tokens for e in encodings])\n",
    "print([e.special_tokens_mask for e in encodings])\n",
    "\n",
    "print(encodings[0].tokens[94:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.get_vocab()['hokkien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(g.LOG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['hello'].shape\n",
    "type(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embedding():\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library\n",
    "\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    model_name = \"glove-wiki-gigaword-{}\".format(g.EMBEDDING_DIMENSION)\n",
    "    glove_model_path = os.path.join(g.DATA_FOLDER, f\"glove_vectors_{g.EMBEDDING_DIMENSION}.txt\")\n",
    "\n",
    "    #if already stored in data, retrieve it \n",
    "    if os.path.exists(glove_model_path): \n",
    "\n",
    "        logger.info('loading embedding vectors (dim = %s) from file',g.EMBEDDING_DIMENSION)\n",
    "        embedding_model = KeyedVectors.load_word2vec_format(glove_model_path, binary=True)\n",
    "    \n",
    "    else:\n",
    "        logger.info('downloading glove model (dim = %s)...',g.EMBEDDING_DIMENSION)\n",
    "        embedding_model : KeyedVectors = gloader.load(model_name)\n",
    "        logger.info('glove loaded')\n",
    "\n",
    "        embedding_model.save_word2vec_format(glove_model_path, binary=True)\n",
    "        logger.info('glove model saved to file in data directory')\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    logger.info('loading time: %f',end_time-start_time)\n",
    "\n",
    "    return embedding_model\n",
    "\n",
    "def build_qg_embedding_matrix(type : str, vocab : dict) -> np.ndarray:\n",
    "\n",
    "    assert type in ['encoder','decoder']\n",
    "    emb_matrix_path = os.path.join(g.DATA_FOLDER, f\"{type}_emb_matrix\")\n",
    "\n",
    "    if os.path.exists(emb_matrix_path): \n",
    "        logger.info('loading embedding matrix from file')\n",
    "        embedding_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
    "    \n",
    "    else : \n",
    "        logger.info('Building embedding matrix...')\n",
    "\n",
    "        emb_model = load_glove_embedding()\n",
    "        assert emb_model is not None, 'WARNING: empty embeddings model'\n",
    "\n",
    "        embedding_dimension = emb_model.vector_size      #how many numbers each emb vector is composed of                                                           \n",
    "        embedding_matrix = np.zeros((len(vocab), embedding_dimension+3), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "        for word, idx in vocab.items():\n",
    "            if idx<4 : continue      #skip the first tokens as they are special tokens \n",
    "            try:\n",
    "                embedding_vector = emb_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "            embedding_matrix[idx] = np.concatenate([embedding_vector,[0,0,0]])    #assign the retrived or the generated vector to the corresponding index \n",
    "        \n",
    "        unk = np.mean(emb_model.vectors, axis=0)\n",
    "        if unk in emb_model.vectors:\n",
    "            unk = np.concatenate([np.random.uniform(low=-0.05, high=0.05,size=embedding_dimension),[0,0,0]])    \n",
    "\n",
    "        embedding_matrix[vocab[g.UNK_TOKEN]] = unk      # add the unk token embedding  \n",
    "\n",
    "        embedding_matrix[vocab[g.PAD_TOKEN],300] = 1.0\n",
    "        embedding_matrix[vocab[g.SOS_TOKEN],301] = 1.0\n",
    "        embedding_matrix[vocab[g.EOS_TOKEN],302] = 1.0\n",
    "\n",
    "        logger.info(f\"Built embedding matrix with shape: {embedding_matrix.shape}\")\n",
    "\n",
    "        np.save(emb_matrix_path,embedding_matrix,allow_pickle=True)\n",
    "        logger.info('embedding matrix saved to file in data directory')\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_embeddings = load_glove_embedding()\n",
    "\n",
    "embedding_matrix = build_qg_embedding_matrix(glove_embeddings, tokenizer.get_vocab())\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "np.concatenate([a,[0,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]],[[4,5,6],[4,5,6],[4,5,6]],[[7,8,9],[7,8,9],[7,8,9]]])\n",
    "\n",
    "b = torch.tensor([[[1,2,3,1],[1,2,3,1],[1,2,3,0]],[[4,5,6,0],[4,5,6,1],[4,5,6,0]],[[7,8,9,1],[7,8,9,1],[7,8,9,1]]])\n",
    "\n",
    "b\n",
    "\n",
    "start = np.array([0,1,0])\n",
    "end = np.array([1,1,2])\n",
    "\n",
    "# c = torch.zeros(a.shape[0],a.shape[1])\n",
    "\n",
    "c = (start[:,None] <= np.arange(a.shape[1])).view('i1')\n",
    "d = (end[:,None] >= np.arange(a.shape[1])).view('i1')\n",
    "\n",
    "f = c*d\n",
    "\n",
    "f = torch.from_numpy(f)\n",
    "\n",
    "f = f.unsqueeze(-1)\n",
    "\n",
    "# c.index_fill_(dim=1,start)\n",
    "\n",
    "torch.cat((a,f),dim=2)\n",
    "\n",
    "# f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('squad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
