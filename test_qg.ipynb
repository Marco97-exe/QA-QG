{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import src.globals as g\n",
    "import src.utils as utils\n",
    "\n",
    "import src.data_handler as handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(g.DATA_FOLDER,'training_set.json')\n",
    "squad_dataset = handling.RawSquadDataset(dataset_path)\n",
    "\n",
    "df = squad_dataset.train_df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = utils.get_Glove_model_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  Tokenizer, Encoding\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD, BertNormalizer\n",
    "from tokenizers.normalizers import Sequence as NormSequence\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=g.UNK_TOKEN))\n",
    "tokenizer.normalizer = BertNormalizer(handle_chinese_chars=False) #NormSequence([NFD(), StripAccents(), Lowercase(), Strip()])    \n",
    "tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    pair=\"[SOS] $A [EOS] [SOS]:1 $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOS]\", 2),\n",
    "        (\"[EOS]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[g.PAD_TOKEN,g.UNK_TOKEN,g.SOS_TOKEN,g.EOS_TOKEN],vocab_size=65000)   #min_frequency\n",
    "\n",
    "l = df.context.to_list() + df.answer.to_list()\n",
    "#l = df.question.to_list()\n",
    "tokenizer.train_from_iterator(l,trainer=trainer) \n",
    "tokenizer.enable_padding(direction=\"right\", pad_id=tokenizer.token_to_id(g.PAD_TOKEN), pad_type_id=1, pad_token=g.PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()\n",
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens([g.PAD_TOKEN,g.UNK_TOKEN]) #,g.SOS_TOKEN,g.EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = df.context.to_list() + df.answer.to_list() \n",
    "s = set()\n",
    "for e in l :\n",
    "    # if 'intellectu' in e:\n",
    "    #     print(e)\n",
    "    s.update(e.split())\n",
    "\n",
    "len(s)\n",
    "        \n",
    "\n",
    "#tokenizer.encode('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?').tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for e in tokenizer.get_vocab().keys() :\n",
    "    if e not in vocab:\n",
    "        # print(e)\n",
    "        n+=1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()[\"tδ\"]\n",
    "\n",
    "df[df['context'].str.contains('tδ')]\n",
    "\n",
    "for e in l :\n",
    "    if 'tδ' in e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)\n",
    "tokenizer.token_to_id('plda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['question_id']=='5726d73d708984140094d310']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = df.loc[49591]\n",
    "r2 = df.loc[49593]\n",
    "s1 = r1['context']\n",
    "s2 = r2['context']\n",
    "s1\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [r1['label_char'][0],r2['label_char'][0]]\n",
    "ends = [r1['label_char'][1],r2['label_char'][1]]\n",
    "\n",
    "starts\n",
    "ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1['answer']\n",
    "r2['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings: list[Encoding] = tokenizer.encode_batch([s1,s2])\n",
    "\n",
    "print([e.ids for e in encodings])\n",
    "print([e.attention_mask for e in encodings])\n",
    "# print([e.offsets for e in encodings])\n",
    "print([e.char_to_token(starts[i]) for i,e in enumerate(encodings)])\n",
    "print([e.char_to_token(ends[i]-1) for i,e in enumerate(encodings)])\n",
    "print([e.type_ids for e in encodings])\n",
    "print([e.tokens for e in encodings])\n",
    "print([e.special_tokens_mask for e in encodings])\n",
    "\n",
    "print(encodings[0].tokens[94:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.get_vocab()['hokkien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(g.LOG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['hello'].shape\n",
    "type(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "np.concatenate([a,[0,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]],[[4,5,6],[4,5,6],[4,5,6]],[[7,8,9],[7,8,9],[7,8,9]]])\n",
    "\n",
    "b = torch.tensor([[[1,2,3,1],[1,2,3,1],[1,2,3,0]],[[4,5,6,0],[4,5,6,1],[4,5,6,0]],[[7,8,9,1],[7,8,9,1],[7,8,9,1]]])\n",
    "\n",
    "\n",
    "start = np.array([0,1,0])\n",
    "end = np.array([1,1,2])\n",
    "\n",
    "c = (start[:,None] <= np.arange(a.shape[1])).view('i1')    #np.less_equal.outer(start, np.arange(a.shape[1])).view('i1')\n",
    "d = (end[:,None] >= np.arange(a.shape[1])).view('i1')\n",
    "c\n",
    "d\n",
    "f = c*d\n",
    "\n",
    "f\n",
    "\n",
    "f = torch.from_numpy(f)\n",
    "\n",
    "f = f.unsqueeze(-1)\n",
    "\n",
    "r = torch.cat((a,f),dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_m = torch.rand((20,5))\n",
    "enc_m[0] = torch.zeros(5)\n",
    "\n",
    "enc_emb = nn.Embedding.from_pretrained(enc_m,padding_idx=0)\n",
    "\n",
    "h_dim = 4\n",
    "\n",
    "rnn = nn.LSTM(5+1, h_dim, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_ids = torch.tensor([[1,2,3,0,0,0],[3,7,8,12,17,19],[3,15,4,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0342, 0.2083, 0.5885, 0.5444, 0.6211],\n",
       "         [0.2240, 0.0550, 0.3989, 0.2854, 0.8079],\n",
       "         [0.0920, 0.3813, 0.9056, 0.2165, 0.2945],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0920, 0.3813, 0.9056, 0.2165, 0.2945],\n",
       "         [0.1063, 0.5565, 0.2262, 0.9885, 0.4189],\n",
       "         [0.2214, 0.5502, 0.4919, 0.6034, 0.9868],\n",
       "         [0.1193, 0.6492, 0.9727, 0.9969, 0.1712],\n",
       "         [0.5893, 0.3778, 0.3170, 0.4766, 0.7206],\n",
       "         [0.3725, 0.2737, 0.1332, 0.8680, 0.7826]],\n",
       "\n",
       "        [[0.0920, 0.3813, 0.9056, 0.2165, 0.2945],\n",
       "         [0.1832, 0.6558, 0.6514, 0.7944, 0.9617],\n",
       "         [0.9532, 0.8505, 0.3287, 0.4101, 0.2038],\n",
       "         [0.0342, 0.2083, 0.5885, 0.5444, 0.6211],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_embeds = enc_emb(ctx_ids)\n",
    "\n",
    "ctx_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0342, 0.2083, 0.5885, 0.5444, 0.6211, 1.0000],\n",
       "         [0.2240, 0.0550, 0.3989, 0.2854, 0.8079, 1.0000],\n",
       "         [0.0920, 0.3813, 0.9056, 0.2165, 0.2945, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0920, 0.3813, 0.9056, 0.2165, 0.2945, 0.0000],\n",
       "         [0.1063, 0.5565, 0.2262, 0.9885, 0.4189, 0.0000],\n",
       "         [0.2214, 0.5502, 0.4919, 0.6034, 0.9868, 1.0000],\n",
       "         [0.1193, 0.6492, 0.9727, 0.9969, 0.1712, 1.0000],\n",
       "         [0.5893, 0.3778, 0.3170, 0.4766, 0.7206, 1.0000],\n",
       "         [0.3725, 0.2737, 0.1332, 0.8680, 0.7826, 0.0000]],\n",
       "\n",
       "        [[0.0920, 0.3813, 0.9056, 0.2165, 0.2945, 0.0000],\n",
       "         [0.1832, 0.6558, 0.6514, 0.7944, 0.9617, 0.0000],\n",
       "         [0.9532, 0.8505, 0.3287, 0.4101, 0.2038, 0.0000],\n",
       "         [0.0342, 0.2083, 0.5885, 0.5444, 0.6211, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.tensor([0,2,3])\n",
    "end = torch.tensor([1,4,3])\n",
    "\n",
    "\n",
    "t1 = torch.le(start.unsqueeze(-1),torch.arange(ctx_embeds.shape[1])).float()\n",
    "t2 = torch.ge(end.unsqueeze(-1),torch.arange(ctx_embeds.shape[1])).float()\n",
    "\n",
    "\n",
    "m = torch.mul(t1,t2).unsqueeze(-1)\n",
    "\n",
    "r = torch.cat((ctx_embeds,m),dim=2)\n",
    "\n",
    "r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2022, -0.1078,  0.0982, -0.0565,  0.0529, -0.1729, -0.0632,\n",
      "          -0.1712],\n",
      "         [-0.2916, -0.1140,  0.1835, -0.0572,  0.0771, -0.1673, -0.0624,\n",
      "          -0.1728],\n",
      "         [-0.3321, -0.1095,  0.2821, -0.0503,  0.1080, -0.1110,  0.0575,\n",
      "          -0.1187],\n",
      "         [-0.2702, -0.1124,  0.2768, -0.0088,  0.2418, -0.1393,  0.0445,\n",
      "          -0.1359],\n",
      "         [-0.2657, -0.1125,  0.2719,  0.0141,  0.2193, -0.1282,  0.0416,\n",
      "          -0.1142],\n",
      "         [-0.2610, -0.1121,  0.2690,  0.0239,  0.1607, -0.0981,  0.0300,\n",
      "          -0.0746]],\n",
      "\n",
      "        [[-0.1948, -0.0824,  0.1723, -0.0545,  0.0888, -0.1376,  0.0804,\n",
      "          -0.0609],\n",
      "         [-0.2262, -0.0842,  0.3153, -0.0096,  0.1223, -0.1930,  0.0737,\n",
      "          -0.0603],\n",
      "         [-0.3226, -0.1003,  0.3598, -0.0157,  0.0318, -0.1474, -0.0390,\n",
      "          -0.0838],\n",
      "         [-0.3730, -0.1423,  0.3673, -0.0218,  0.0724, -0.1046,  0.0315,\n",
      "           0.0210],\n",
      "         [-0.3900, -0.1090,  0.4555,  0.0242,  0.1192, -0.1807, -0.0273,\n",
      "          -0.0605],\n",
      "         [-0.3013, -0.0493,  0.4748,  0.0790,  0.1184, -0.1996,  0.0252,\n",
      "          -0.0778]],\n",
      "\n",
      "        [[-0.1948, -0.0824,  0.1723, -0.0545,  0.0813, -0.1317,  0.0795,\n",
      "          -0.0521],\n",
      "         [-0.2591, -0.0590,  0.3571, -0.0406,  0.0690, -0.1810,  0.0752,\n",
      "          -0.0450],\n",
      "         [-0.4146, -0.0733,  0.4654,  0.0392,  0.1926, -0.1086,  0.0502,\n",
      "           0.0684],\n",
      "         [-0.3138, -0.1049,  0.3763,  0.0222,  0.0852, -0.1454, -0.0391,\n",
      "          -0.1554],\n",
      "         [-0.2924, -0.1016,  0.3345,  0.0363,  0.2193, -0.1282,  0.0416,\n",
      "          -0.1142],\n",
      "         [-0.2772, -0.1038,  0.3123,  0.0384,  0.1607, -0.0981,  0.0300,\n",
      "          -0.0746]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out, _ = rnn(r)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7264e-01, -2.1914e-01,  3.2841e-01, -7.9039e-02,  1.9619e-01,\n",
       "          -3.0905e-01, -5.5260e-02, -3.0239e-01],\n",
       "         [-5.6204e-01, -2.2537e-01,  4.1372e-01, -7.9720e-02,  2.2043e-01,\n",
       "          -3.0340e-01, -5.4385e-02, -3.0400e-01],\n",
       "         [-6.0256e-01, -2.2093e-01,  5.1239e-01, -7.2817e-02,  2.5131e-01,\n",
       "          -2.4718e-01,  6.5463e-02, -2.4992e-01],\n",
       "         [-5.4063e-01, -2.2383e-01,  5.0708e-01, -3.1315e-02,  3.8510e-01,\n",
       "          -2.7544e-01,  5.2449e-02, -2.6716e-01],\n",
       "         [-5.3616e-01, -2.2388e-01,  5.0219e-01, -8.3749e-03,  3.6259e-01,\n",
       "          -2.6428e-01,  4.9616e-02, -2.4544e-01],\n",
       "         [-5.3144e-01, -2.2346e-01,  4.9923e-01,  1.3773e-03,  3.0402e-01,\n",
       "          -2.3419e-01,  3.7963e-02, -2.0588e-01]],\n",
       "\n",
       "        [[-4.9615e-01, -1.7703e-01,  5.2979e-01, -5.4246e-02,  1.8101e-01,\n",
       "          -2.9813e-01,  1.0448e-01, -1.1465e-01],\n",
       "         [-5.2755e-01, -1.7875e-01,  6.7285e-01, -9.2843e-03,  2.1446e-01,\n",
       "          -3.5345e-01,  9.7764e-02, -1.1398e-01],\n",
       "         [-6.2392e-01, -1.9491e-01,  7.1727e-01, -1.5388e-02,  1.2396e-01,\n",
       "          -3.0792e-01, -1.4891e-02, -1.3757e-01],\n",
       "         [-6.7430e-01, -2.3685e-01,  7.2483e-01, -2.1548e-02,  1.6457e-01,\n",
       "          -2.6505e-01,  5.5601e-02, -3.2701e-02],\n",
       "         [-6.9135e-01, -2.0360e-01,  8.1306e-01,  2.4453e-02,  2.1142e-01,\n",
       "          -3.4119e-01, -3.2270e-03, -1.1427e-01],\n",
       "         [-6.0261e-01, -1.4394e-01,  8.3235e-01,  7.9219e-02,  2.1059e-01,\n",
       "          -3.6006e-01,  4.9322e-02, -1.3151e-01]],\n",
       "\n",
       "        [[-4.8681e-01, -1.6995e-01,  5.0859e-01, -4.7687e-02,  2.1596e-01,\n",
       "          -2.6381e-01,  1.1911e-01, -1.1422e-01],\n",
       "         [-5.5103e-01, -1.4650e-01,  6.9342e-01, -3.3755e-02,  2.0364e-01,\n",
       "          -3.1317e-01,  1.1476e-01, -1.0721e-01],\n",
       "         [-7.0656e-01, -1.6085e-01,  8.0174e-01,  4.5976e-02,  3.2727e-01,\n",
       "          -2.4079e-01,  8.9815e-02,  6.2092e-03],\n",
       "         [-6.0582e-01, -1.9244e-01,  7.1262e-01,  2.9059e-02,  2.1991e-01,\n",
       "          -2.7755e-01,  4.6700e-04, -2.1759e-01],\n",
       "         [-5.8434e-01, -1.8906e-01,  6.7078e-01,  4.3092e-02,  3.5396e-01,\n",
       "          -2.6031e-01,  8.1204e-02, -1.7638e-01],\n",
       "         [-5.6916e-01, -1.9126e-01,  6.4861e-01,  4.5221e-02,  2.9539e-01,\n",
       "          -2.3021e-01,  6.9552e-02, -1.3682e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape\n",
    "v = torch.mean(out,dim=1)\n",
    "v.shape\n",
    "v = v.unsqueeze(1).expand(out.size())\n",
    "\n",
    "torch.add(v,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(out,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros((3,5,10))\n",
    "\n",
    "c = torch.rand(3,10)\n",
    "\n",
    "b.shape\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3278, 0.2552, 0.4009, 0.8780, 0.1698, 0.7730, 0.6941, 0.6092, 0.2546,\n",
       "         0.3166],\n",
       "        [0.4466, 0.7695, 0.3473, 0.1396, 0.9413, 0.7366, 0.5315, 0.9089, 0.8274,\n",
       "         0.3054],\n",
       "        [0.1927, 0.0131, 0.3705, 0.5231, 0.0566, 0.2250, 0.9373, 0.5516, 0.4360,\n",
       "         0.7231]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.3278, 0.2552, 0.4009, 0.8780, 0.1698, 0.7730, 0.6941, 0.6092,\n",
       "          0.2546, 0.3166],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.4466, 0.7695, 0.3473, 0.1396, 0.9413, 0.7366, 0.5315, 0.9089,\n",
       "          0.8274, 0.3054],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1927, 0.0131, 0.3705, 0.5231, 0.0566, 0.2250, 0.9373, 0.5516,\n",
       "          0.4360, 0.7231],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,1,:] = c\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_ids = torch.tensor([[1,2,0],[8,12,17],[1,0,0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_embeds = enc_emb(answ_ids)\n",
    "\n",
    "answ_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(answ_embeds.shape[0],answ_embeds.shape[1],h_dim*2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(answ_embeds.shape[0]):\n",
    "    z[i,0:end[i]+1-start[i],:] = out[i,start[i]:end[i]+1,:]\n",
    "\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((z,answ_embeds),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[11,2,3],[1,2,3]],[[4,5,6],[43,5,6],[4,5,6]]])\n",
    "b = torch.tensor([[[2,3,4],[2,3,4],[2,3,4]],[[5,6,7],[5,6,7],[5,6,7]]])\n",
    "\n",
    "c = (a,b)\n",
    "\n",
    "tuple((torch.cat((hidden[0:hidden.size(0):2], hidden[1:hidden.size(0):2]), dim=2) for hidden in c))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('squad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
