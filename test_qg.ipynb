{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import src.globals as g\n",
    "import src.utils as utils\n",
    "\n",
    "import src.data_handler as handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(g.DATA_FOLDER,'training_set.json')\n",
    "squad_dataset = handling.RawSquadDataset(dataset_path)\n",
    "\n",
    "df = squad_dataset.train_df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = utils.get_Glove_model_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import  Tokenizer, Encoding\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD, BertNormalizer\n",
    "from tokenizers.normalizers import Sequence as NormSequence\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=g.UNK_TOKEN))\n",
    "tokenizer.normalizer = BertNormalizer(handle_chinese_chars=False) #NormSequence([NFD(), StripAccents(), Lowercase(), Strip()])    \n",
    "tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    pair=\"[SOS] $A [EOS] [SOS]:1 $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[SOS]\", 2),\n",
    "        (\"[EOS]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[g.PAD_TOKEN,g.UNK_TOKEN,g.SOS_TOKEN,g.EOS_TOKEN],vocab_size=65000)   #min_frequency\n",
    "\n",
    "l = df.context.to_list() + df.answer.to_list()\n",
    "#l = df.question.to_list()\n",
    "tokenizer.train_from_iterator(l,trainer=trainer) \n",
    "tokenizer.enable_padding(direction=\"right\", pad_id=tokenizer.token_to_id(g.PAD_TOKEN), pad_type_id=1, pad_token=g.PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()\n",
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens([g.PAD_TOKEN,g.UNK_TOKEN]) #,g.SOS_TOKEN,g.EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = df.context.to_list() + df.answer.to_list() \n",
    "s = set()\n",
    "for e in l :\n",
    "    # if 'intellectu' in e:\n",
    "    #     print(e)\n",
    "    s.update(e.split())\n",
    "\n",
    "len(s)\n",
    "        \n",
    "\n",
    "#tokenizer.encode('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?').tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for e in tokenizer.get_vocab().keys() :\n",
    "    if e not in vocab:\n",
    "        # print(e)\n",
    "        n+=1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()[\"tδ\"]\n",
    "\n",
    "df[df['context'].str.contains('tδ')]\n",
    "\n",
    "for e in l :\n",
    "    if 'tδ' in e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(2)\n",
    "tokenizer.token_to_id('plda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['question_id']=='5726d73d708984140094d310']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = df.loc[49591]\n",
    "r2 = df.loc[49593]\n",
    "s1 = r1['context']\n",
    "s2 = r2['context']\n",
    "s1\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [r1['label_char'][0],r2['label_char'][0]]\n",
    "ends = [r1['label_char'][1],r2['label_char'][1]]\n",
    "\n",
    "starts\n",
    "ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1['answer']\n",
    "r2['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings: list[Encoding] = tokenizer.encode_batch([s1,s2])\n",
    "\n",
    "print([e.ids for e in encodings])\n",
    "print([e.attention_mask for e in encodings])\n",
    "# print([e.offsets for e in encodings])\n",
    "print([e.char_to_token(starts[i]) for i,e in enumerate(encodings)])\n",
    "print([e.char_to_token(ends[i]-1) for i,e in enumerate(encodings)])\n",
    "print([e.type_ids for e in encodings])\n",
    "print([e.tokens for e in encodings])\n",
    "print([e.special_tokens_mask for e in encodings])\n",
    "\n",
    "print(encodings[0].tokens[94:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.get_vocab()['hokkien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(g.LOG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['hello'].shape\n",
    "type(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "np.concatenate([a,[0,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]],[[4,5,6],[4,5,6],[4,5,6]],[[7,8,9],[7,8,9],[7,8,9]]])\n",
    "\n",
    "b = torch.tensor([[[1,2,3,1],[1,2,3,1],[1,2,3,0]],[[4,5,6,0],[4,5,6,1],[4,5,6,0]],[[7,8,9,1],[7,8,9,1],[7,8,9,1]]])\n",
    "\n",
    "\n",
    "start = np.array([0,1,0])\n",
    "end = np.array([1,1,2])\n",
    "\n",
    "c = (start[:,None] <= np.arange(a.shape[1])).view('i1')    #np.less_equal.outer(start, np.arange(a.shape[1])).view('i1')\n",
    "d = (end[:,None] >= np.arange(a.shape[1])).view('i1')\n",
    "c\n",
    "d\n",
    "f = c*d\n",
    "\n",
    "f\n",
    "\n",
    "f = torch.from_numpy(f)\n",
    "\n",
    "f = f.unsqueeze(-1)\n",
    "\n",
    "r = torch.cat((a,f),dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_m = torch.rand((20,5))\n",
    "enc_m[0] = torch.zeros(5)\n",
    "\n",
    "enc_emb = nn.Embedding.from_pretrained(enc_m,padding_idx=0)\n",
    "\n",
    "h_dim = 4\n",
    "\n",
    "rnn = nn.LSTM(5+1, h_dim, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_ids = torch.tensor([[1,2,3,0,0,0],[3,7,8,12,17,19],[3,15,4,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_embeds = enc_emb(ctx_ids)\n",
    "\n",
    "ctx_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.tensor([0,2,3])\n",
    "end = torch.tensor([1,4,3])\n",
    "\n",
    "\n",
    "t1 = torch.le(start.unsqueeze(-1),torch.arange(ctx_embeds.shape[1])).float()\n",
    "t2 = torch.ge(end.unsqueeze(-1),torch.arange(ctx_embeds.shape[1])).float()\n",
    "\n",
    "\n",
    "m = torch.mul(t1,t2).unsqueeze(-1)\n",
    "\n",
    "r = torch.cat((ctx_embeds,m),dim=2)\n",
    "\n",
    "r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = rnn(r)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape\n",
    "v = torch.mean(out,dim=1)\n",
    "v.shape\n",
    "v = v.unsqueeze(1).expand(out.size())\n",
    "\n",
    "torch.add(v,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(out,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_ids = torch.tensor([[1,2,0],[8,12,17],[1,0,0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_embeds = enc_emb(answ_ids)\n",
    "\n",
    "answ_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(answ_embeds.shape[0],answ_embeds.shape[1],h_dim*2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(answ_embeds.shape[0]):\n",
    "    z[i,0:end[i]+1-start[i],:] = out[i,start[i]:end[i]+1,:]\n",
    "\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((z,answ_embeds),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answ_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3],[11,2,3],[1,2,3]],[[4,5,6],[43,5,6],[4,5,6]]])\n",
    "b = torch.tensor([[[2,3,4],[2,3,4],[2,3,4]],[[5,6,7],[5,6,7],[5,6,7]]])\n",
    "\n",
    "c = (a,b)\n",
    "\n",
    "tuple((torch.cat((hidden[0:hidden.size(0):2], hidden[1:hidden.size(0):2]), dim=2) for hidden in c))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('squad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
