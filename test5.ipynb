{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import src.utils as utils \n",
    "import src.globals as globals\n",
    "import src.data_handler as handling\n",
    "\n",
    "from datasets import Dataset \n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(globals.DATA_FOLDER,'training_set.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = utils.load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = handling.RawSquadDataset(dataset_path)\n",
    "\n",
    "df = squad_dataset.train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(globals.DATA_FOLDER,'bert-base-uncased-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer = BertWordPieceTokenizer(vocab_path, lowercase=True)\n",
    "wp_tokenizer.enable_padding(direction=\"right\", pad_type_id=1)\n",
    "wp_tokenizer.enable_truncation(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Encoding\n",
    "\n",
    "def transform(batch):\n",
    "\n",
    "    encodings: list[Encoding] = wp_tokenizer.encode_batch(list(zip(batch['question'],batch['context'])))\n",
    "\n",
    "    starts = list(map(lambda x: x[0],batch['label_char']))\n",
    "    ends = list(map(lambda x: x[1],batch['label_char']))\n",
    "\n",
    "    encodings = {\n",
    "        'ids': [e.ids for e in encodings],\n",
    "        'mask': [e.attention_mask for e in encodings],\n",
    "        'offsets': [e.offsets for e in encodings], \n",
    "        'sequence_ids': [e.sequence_ids for e in encodings],\n",
    "        'type_ids': [e.type_ids for e in encodings],\n",
    "        'context_text': batch['context'],\n",
    "        'question_text': batch['question'],\n",
    "        'answer_text': batch['answer'],\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "hf_dataset.set_transform(transform,output_all_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_dataset[4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = hf_dataset[57912]\n",
    "start_token = ex['label_token_start']\n",
    "end_token = ex['label_token_end']\n",
    "start_char = ex['offsets'][start_token][0]\n",
    "end_char = ex['offsets'][end_token][1]\n",
    "\n",
    "print(start_char)\n",
    "print(end_char)\n",
    "\n",
    "ex['context_text'][start_char:end_char]\n",
    "ex['context_text'][ex['label_char_start']:ex['label_char_end']]\n",
    "ex['answer_text']\n",
    "\n",
    "len(ex['context_ids']) == len(ex['context_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_c = ex['label_char_start']\n",
    "end_c = ex['label_char_end']\n",
    "\n",
    "starts, ends = zip(*ex['offsets'])\n",
    "\n",
    "try :\n",
    "    start_idx = starts.index(start_c)\n",
    "except :\n",
    "    print('errore start')\n",
    "\n",
    "try: \n",
    "    end_idx = ends.index(end_c)\n",
    "except :\n",
    "    print('errore end')\n",
    "\n",
    "\n",
    "ex['context_tokens'][start_idx] == ex['answer_tokens'][0]\n",
    "ex['context_tokens'][end_idx] == ex['answer_tokens'][-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d27d3bb0e70e29a6993d11ced729f39970904d8c590c3a159e115fe96c0c042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('squad': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
